Given two distributions with variances $ \sigma _1 ^2 , \sigma _2 ^2 , $
this is a test of the null hypothesis that their means differ by
$ \mu . $ We have two samples $x_{i}$ and $y_{i}$ of sizes m and n. The
distribution used is N(0,1); the test statistic is
$$\frac{ ( \bar{x} - \bar{y} ) - \mu }
{ \sqrt {\frac{\sigma _1 ^2 }{m} + \frac{\sigma _1 ^2}{n} } } .$$

If the two distributions are not normal, or if the variances have to be
estimated, the test is approximate and large samples should be taken.

Special cases of this test set the variances equal and/or the difference
in means equal to zero.
